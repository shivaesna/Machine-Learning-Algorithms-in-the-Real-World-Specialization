{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>This is a draft version and the notebook is due to be changed and finalized soon.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello again! You are going to implement a logistic regression classifer in this Jupyter notebook using scikit-learn and predict using it. We will also see a technique which is useful for visualizing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order for the notebooks to function as intended, modify only between lines marked \"### begin your code here (__ lines).\" and \"### end your code here.\". \n",
    "\n",
    "- The line count is a suggestion of how many lines of code you need to accomplish what is asked.\n",
    "\n",
    "- You should execute the cells (the boxes that a notebook is composed of) in order.\n",
    "\n",
    "- You can execute a cell by pressing Shift and Enter (or Return) simultaneously.\n",
    "\n",
    "- You should have completed the previous Jupyter notebooks before attempting this one as the concepts covered there are not repeated, for the sake of brevity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the appropriate packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing new here. We will import logistic regression class along with some helpers from scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn off the scientific notation for floating point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and examining the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load our data from a CSV file and put it in a pandas an object of the `DataFrame` class.\n",
    "\n",
    "This dataset is the breast cancer Wisconsin (diagnostic) dataset which contains 30 different features computed from a images of a fine needle aspirate (FNA) of breast masses for 569 patients with each example labeled as being a _benign_ or _malignant_ mass.\n",
    "\n",
    "* This was taken and modified from the Machine Learning dataset repository of School of Information and Computer Science of University of California Irvine (UCI):\n",
    " \n",
    "> _Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_30 = pd.read_csv('data_logistic_regression.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example to be educational, we need to be able to visualize our data, so our data has to be 2-dimensional. However, our data here is 30 dimensional. Let us use a trick (that we can use for many things including visualizations) to get 2-dimensional data out of this dataset.\n",
    "\n",
    "Remember we talked about _unsupervised learning_ in course 1. We said that _representation learning_, the methods use to create representations of the data (which are hopefully helping us to dod machine learning more efficiently) are a subclass of unsupervised learning methods. Specifically, we said that _dimensionality reduction_ are a set of representation learning algorithms aimed at, as the name suggests, reducing the dimensionality of our data. We are going to use a very popular dimensionality reduction technique, called the _Principal Components Analysis_ (_PCA_) to reduce the dimensioanlity of our feature space down to 2, so we can visualize our data in 3D plots.\n",
    "\n",
    "Note that we can not only expand our feature space by adding features, for exmample, nonlinear feature expansions, but also transform features and get new ones and we are doing exactly that with PCA. We are taking all of the features and constructing the two features that are _a._ a linear combination of our features; and _b._ are most informative in spreading out the data. In other words, with PCA, we construct two features from our original features where in these new features, the data points are most spread out and varied, among all features we can construct out of linearly combining our original features.\n",
    "\n",
    "To do that we first need to extract our data, from the dataframe, in NumPy arrays: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_30 = df_30.drop('type', axis=1).to_numpy()\n",
    "y_text = df_30['type'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's check `X_30`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_30.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same thing for `y_text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and for shape of `y_text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_30)\n",
    "X = pca.transform(X_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how now we can find the proper transformation from the `X_30` and specify that we want our transformation to produce data with 2 features for us in the output by letting `n_components=2`? Also, see how PCA does not get the labels, in `fit`? It's unsupervised learning after all and it does not use the labels!\n",
    "\n",
    "Let's check this new `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and its shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate a data frame from this two dimesional data `X` that we generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=np.c_[X, y_text], columns=['Feature 1', 'Feature 2', 'Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our new 2-dimensional data as a table. We have to construct a data frame from our new 2-dimensional data as well as our labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also do a scatter plot of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x='Feature 1', y='Feature 2', color='Label')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create $\\{-1, +1\\}$ labels for our data from `y_text` and assign it to (vector) variable `y`. We use `LabelEncoder` from scikit-learn again to transform labels into -1s or +1s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (2 * LabelEncoder().fit_transform(y_text)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual let's check our `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and its shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot our training data in 3D with a 3D scatter plot (we are going to use surface plots afterwards and the new interface of plotly cannot do surface plots yet, so we are using the older style rather than plotly express):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_colorscale = [\n",
    "                     [0.0, 'rgb(239, 85, 59)'],\n",
    "                     [1.0, 'rgb(99, 110, 250)'],\n",
    "                    ]\n",
    "\n",
    "layout = go.Layout(scene=dict(\n",
    "                              xaxis=dict(title='Feature 1'),\n",
    "                              yaxis=dict(title='Featrue 2'),\n",
    "                              zaxis=dict(title='Label')\n",
    "                             ),\n",
    "                  )\n",
    "\n",
    "points = go.Scatter3d(x=df['Feature 1'], \n",
    "                      y=df['Feature 2'], \n",
    "                      z=y,\n",
    "                      mode='markers',\n",
    "                      text=df['Label'],\n",
    "                      marker=dict(\n",
    "                                  size=3,\n",
    "                                  color=y,\n",
    "                                  colorscale=points_colorscale\n",
    "                            ),\n",
    "                     )\n",
    "\n",
    "fig2 = go.Figure(data=[points], layout=layout)\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split our data into training, validation and test sets. We don't need validation data in this example and we won't be doing model selection here. So, let's use 70% and 30% for training test data, repectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and visualizing a logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our logistic regression model then by creating an object of the `LogisticRegression` class and assign the name `logreg` to the resulting object. \n",
    "\n",
    "You can see the documentation for `LogisticRegression` here:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "Go ahead and do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### begin your code here (1 line).\n",
    "\n",
    "### end your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fit `logreg` to `X_train` and `y_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### begin your code here (1 line).\n",
    "\n",
    "### end your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will get a summary for the model:\n",
    "\n",
    "> LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    ">           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
    ">           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
    ">           tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "* You may also get a warning because you have not explicitly set a solver and that is going to change in newer versions of scikit-learn. Nothing you should be worried about here.\n",
    "\n",
    "Let's visualize the surface generated by our logistic regression model. First, we need to generate a number of points required for creating a visualization of the decision surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail_steps = 100\n",
    "\n",
    "(x_vis_0_min, x_vis_1_min) = X_train.min(axis=0)\n",
    "(x_vis_0_max, x_vis_1_max) = X_train.max(axis=0)\n",
    "\n",
    "x_vis_0_range = np.linspace(x_vis_0_min, x_vis_0_max, detail_steps)\n",
    "x_vis_1_range = np.linspace(x_vis_1_min, x_vis_1_max, detail_steps)\n",
    "\n",
    "(XX_vis_0, XX_vis_1) = np.meshgrid(x_vis_0_range, x_vis_0_range)\n",
    "\n",
    "X_vis = np.c_[XX_vis_0.reshape(-1), XX_vis_1.reshape(-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to predict the proability associated with points in this generated data in order to visualize it. You can get the probabilities associated with belonging to classes by `predict_proba` method. Let's use that to calculate probabilities for points in `X_vis`. Use `predict_proba` just like `predict` to predict probabilities instead of actual classes. Go ahead and do that now, and assign the result to variable `probs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### begin your code here (1 line).\n",
    "\n",
    "### end your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape of this variable `probs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it has two column, because it gives the probability of belonging to each of the two classes. However, we care only about the probability of belonging to the positive class, so we can only choose the cloumn with index `1`. Also, the probabilities will be in $[0,1]$ while our labels are $\\{+1, 1\\}$, so we will transform the probabilities to be in range $[-1, +1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_vis = (2 * probs[:, 1]) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can transfrom `yhat_vis` into the shape required for a surface plot and plot away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YYhat_vis = yhat_vis.reshape(XX_vis_0.shape)\n",
    "\n",
    "surface_colorscale = [\n",
    "                      [0.0, 'rgb(235, 185, 177)'],\n",
    "                      [1.0, 'rgb(199, 204, 249)'],\n",
    "                     ]\n",
    "\n",
    "surface = go.Surface(\n",
    "                     x=XX_vis_0, \n",
    "                     y=XX_vis_1,\n",
    "                     z=YYhat_vis,\n",
    "                     colorscale=surface_colorscale,\n",
    "                     showscale=False\n",
    "                    )\n",
    "\n",
    "fig3 = go.Figure(data=[points, surface], layout=layout)\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that logistic regression has fit a surface to our data that is has the logistic (or Sigmoid) function as its intersection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our accuracies next. First, the training accuracy. For that let's get the predictions of training data. Predict `yhat_train` by `logreg` on `X_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### begin your code here (1 line).\n",
    "\n",
    "### end your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(yhat_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got ??.??%. Let's check accuracy on the test data. Predict `yhat_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### begin your code here (1 line).\n",
    "\n",
    "### end your code here.\n",
    "accuracy_score(yhat_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "??.??%. We have better performance on test data than on training data! But that's just random and it does not mean that we have perfectly generalized and have no overfitting: that is theoretically impossible!\n",
    "\n",
    "That's it for now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
