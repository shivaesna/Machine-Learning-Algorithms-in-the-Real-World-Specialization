{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>This is a draft version and the notebook is incomplete and the statements do not match the data at this point. It will be fixed soon.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $k$-Nearest Neighbours ($k$-NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome! In this Jupyter notebook, you will use scikit-learn to build and predict using the $k$-Nearest Neighbour (or $k$-NN) algorithm. We will also talk distance measures, a bit. But that's enough, let's jump into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order for the notebooks to function as intended, modify only between lines marked \"### begin your code here (__ lines).\" and \"### end your code here.\". \n",
    "\n",
    "- The line count is a suggestion of how many lines of code you need to accomplish what is asked.\n",
    "\n",
    "- You should execute the cells (the boxes that a notebook is composed of) in order.\n",
    "\n",
    "- You can execute a cell by pressing Shift and Enter (or Return) simultaneously.\n",
    "\n",
    "- You should have completed the _Decision Trees_ Jupyter notebook before attempting this one as the concepts covered there are not repeated, for the sake of brevity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the appropriate packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we are loading required packages. From scikit-learn, we will load the packages appropriate for k-NNs this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn off the scientific notation for floating point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and examining the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load our data from a CSV file and put it in a pandas an object of the `DataFrame` class.\n",
    "\n",
    "This is the Iris flower dataset, a very famous dataset which was published in 1936 by studying three different species of the flower Iris: _Iris setosa_, _Iris versicolor_ and _Iris virginica_. Originally, the dataset has 150 examples which corresponds to 150 different Iris flowers measured (50 flowers from each species). The dataset has 4 features, the sepal length, sepal width, petal length and petal width of each flower in centimeters. However, for the purpose of ease of illustration, we have chosen only two of the features: the sepal length and sepal width.\n",
    "\n",
    "* This was taken and modified from the Machine Learning dataset repository of School of Information and Computer Science of University of California Irvine (UCI):\n",
    " \n",
    "> _Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_knn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our data as a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because now our data has 2 features, we can use a single scatter plot to take a look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x=\"Sepal Length\", y=\"Sepal Width\", color=\"Species\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract our data and targets as NumPy arrays `X` and `y`, from pandas data frame `df`. This time, for visualization purposes, we need our targets `y` to be integers instead of text, so we have to extract text labels as `y_text` and then transform them into integer labels `y`. Fortunately, we can do that with the `LabelEncoder` class that comes in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Species', axis=1).to_numpy()\n",
    "y_text = df['Species'].to_numpy()\n",
    "y = LabelEncoder().fit_transform(y_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our data `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape of `X`as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same checks with the targets vector `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the shape of `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's split our data into training, validation and test sets. Let's use 60% (90 examples) for training, 20% for validation (30 examples) and the remaining 20% (30 examples) as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_vt, y_train, y_vt) = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "(X_validation, X_test, y_validation, y_test) = train_test_split(X_vt, y_vt, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and visualizing a $k$-NN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build our $k$-NN model! We will create an object of the class `KNeighborsClassifier` and assign the name `knn` for the resulting object. Remember that we have to specify a number for $k$ which is called `n_neighbors` in scikit-learn implementation.\n",
    "\n",
    "However, before doing that let's have a quick discussion about distance measures which greatly affect how our $k$-NN classifier performs: Remember that you have to use your knowledge of the data to come up with a distance measure that makes sense for the data you have. You also may have to scale different features by different weights to get a good diatnce measurement out of their combination, before sttempting to train and use a model. For example, if we have integer data, it may make sense to use a Manhattan distance measure. Or use Hamming distance for bit data.\n",
    "\n",
    "In our data here, all our measure ments are of length and are in centimeters, so no adjustment is needed and a Euclidian distance measure actually makes a lot of sense, since it is measuring some kind of length and the unweighted combination is also sound. For `KNeighborsClassifier`, the default distance measure or `metric` (as the argument is called in scikit-learn) is the $L_p$ measure or the Minkowski distance (`metric=minkowski` in scikit-learn call) with $p=2$ (`p=2` in `KNeighborsClassifier` arguments), which is nothing other than the $L_2$ distance measure or the Eucliadian distance.\n",
    "\n",
    "So, other than setting `n_neighbors` to a suitable value, we don't have to specify anything else. Let's start with a value of 1 for `n_neighbors`. \n",
    "\n",
    "You can see the documentation for `KNeighborsClassifier` here:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    "Go ahead and implement that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### begin your code here (1 line).\n",
    "\n",
    "### end your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's fit our `knn` to our `X_train` and `y_train`. This does nothing but store the training example as $k$-NN is \"lazy\": It does all calculations as prediction time and by measuring the distance from the operational datapoints provided (whose labels have to be predicted) to each of the training datapoints, finding the closest training datapoints to the operational points, looking at the labels for those closest training datapoints, and finding the majority class among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### begin your code here (1 line).\n",
    "\n",
    "### end your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you will get a summary for the model:\n",
    "\n",
    "> KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    ">            metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
    ">            weights='uniform')\n",
    "\n",
    "Now, let's visualize our $k$-NN model. We are using plotly heatmaps (which are a bit more tedious to produce at this moment in time as they have not been ported to plotly express yet) to show regions where points will be predicted in different classes (this will take some time) and we will overlay a scatter plot of training points on top of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail_steps = 500\n",
    "\n",
    "(x_vis_0_min, x_vis_0_max) = (X[:, 0].min() - 0.5, X[:, 0].max() + 0.5)\n",
    "(x_vis_1_min, x_vis_1_max) = (X[:, 1].min() - 0.5, X[:, 1].max() + 0.5)\n",
    "\n",
    "x_vis_0_range = np.linspace(x_vis_0_min, x_vis_0_max, detail_steps)\n",
    "x_vis_1_range = np.linspace(x_vis_1_min, x_vis_1_max, detail_steps)\n",
    "\n",
    "(XX_vis_0, XX_vis_1) = np.meshgrid(x_vis_0_range, x_vis_1_range)\n",
    "X_vis = np.c_[XX_vis_0.reshape(-1), XX_vis_1.reshape(-1)]\n",
    "\n",
    "yhat_vis = knn.predict(X_vis)\n",
    "YYhat_vis = yhat_vis.reshape(XX_vis_0.shape)\n",
    "\n",
    "region_colorscale = [\n",
    "                     [0.0, 'rgb(199, 204, 249)'],\n",
    "                     [0.5, 'rgb(235, 185, 177)'],\n",
    "                     [1.0, 'rgb(159, 204, 186)']\n",
    "                    ]\n",
    "points_colorscale = [\n",
    "                     [0.0, 'rgb(99, 110, 250)'],\n",
    "                     [0.5, 'rgb(239, 85, 59)'],\n",
    "                     [1.0, 'rgb(66, 204, 150)']\n",
    "                    ]\n",
    "fig2 = go.Figure(\n",
    "                data=[\n",
    "                      go.Heatmap(x=x_vis_0_range,\n",
    "                                 y=x_vis_1_range,\n",
    "                                 z=YYhat_vis,\n",
    "                                 colorscale=region_colorscale,\n",
    "                                 showscale=False),\n",
    "                      go.Scatter(x=df['Sepal Length'], \n",
    "                                 y=df['Sepal Width'],\n",
    "                                 mode='markers',\n",
    "                                 text=df['Species'],\n",
    "                                 name='',\n",
    "#                                 showscale=False,\n",
    "                                 marker=dict(\n",
    "                                             color=y,\n",
    "                                             colorscale=points_colorscale\n",
    "                                            )\n",
    "                                )\n",
    "                     ],\n",
    "                     layout=go.Layout(\n",
    "                                      xaxis=dict(title='Sepal Length'),\n",
    "                                      yaxis=dict(title='Sepal Width')\n",
    "                                     )\n",
    "               )\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and asessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation time! Let's first see how does or $k$-NN does on training data. We expect to get an accuracy result of near 100% or 1.0. Not exactly 100% because the algorihthm in scikit-learn does not count the same point as a neighbour. Otherwise, we would have had a perfect 100% since the closest single point to each training point in the training points is that point itself and the classes predicted will match that of the training targets. So go ahead and calculate `yhat_train` using `knn` and `X_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### begin your code here (1 line).\n",
    "\n",
    "### end your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to measure the accuracy from `yhat_train` and `y_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(yhat_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is ??.??%. It is as we expected. And we also expect the accuracy on validation points not be that high, since we are probably overfitting by using $k=1$. Put in the line that generates `yhat_validation`, so we can measure the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### begin your code here (1 line).\n",
    "\n",
    "### end your code here.\n",
    "accuracy_score(yhat_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we got ??.??% which has a big gap with the accuracy on training data.\n",
    "\n",
    "Let's use a higher value of $k$ or `n_neighbors`. Let's use 3 and see what happens. Go ahead a make a new model `knn3` and provide `X_train` and `y_train` to it afterwards using the methd `fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### begin your code here (2 lines).\n",
    "\n",
    "\n",
    "### end your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model summary should indicate `n_neighbors=3` now:\n",
    "\n",
    "> KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    ">            metric_params=None, n_jobs=None, **n_neighbors=3**, p=2,\n",
    ">            weights='uniform')\n",
    "\n",
    "We can visualize the model with $k=3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat3_vis = knn3.predict(X_vis)\n",
    "YYhat3_vis = yhat3_vis.reshape(XX_vis_0.shape)\n",
    "\n",
    "fig3 = fig2\n",
    "fig3['data'][0]['z'] = YYhat3_vis\n",
    "\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the regions are smoother and less \"patchy\".\n",
    "\n",
    "Now, predict `yhat_train3` with this new `knn3`, so we can the accuracy on training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### begin your code here (1 line).\n",
    "\n",
    "### end your code here.\n",
    "accuracy_score(yhat_train3, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got ??.??% This time we did not predict to near 100% accuracy since the training points surrounding a training point may have different labels. On to validation accuracy then. Let's predict `yhat_validation3` and we can the accuracy on validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### begin your code here (1 line).\n",
    "\n",
    "### end your code here.\n",
    "accuracy_score(yhat_validation3, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a better accuracy on validation points (??.??%), again as expected, since we are probably overfitting less. Let's try a $k=5$ as well. Make a new model `knn5` with `n_neighbors=5` and provide the data to it via `fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### begin your code here (2 lines).\n",
    "\n",
    "\n",
    "### end your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:\n",
    "\n",
    "> KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    ">            metric_params=None, n_jobs=None, **n_neighbors=5**, p=2,\n",
    ">            weights='uniform')\n",
    "           \n",
    "We can visualize the model with $k=5$ as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat5_vis = knn5.predict(X_vis)\n",
    "YYhat5_vis = yhat3_vis.reshape(XX_vis_0.shape)\n",
    "\n",
    "fig4 = fig2\n",
    "fig4['data'][0]['z'] = YYhat5_vis\n",
    "\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the regions are even more smooth.\n",
    "\n",
    "Next, let's evaluate our model. First on training data. fill in the part that calculates `yhat_train5` and we can measure the accuracy on training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### begin your code here (1 line).\n",
    "\n",
    "### end your code here.\n",
    "accuracy_score(yhat_train5, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "??.??% Do the same on validation data by predicting `yhat_validation5`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### begin your code here (1 line).\n",
    "\n",
    "### end your code here.\n",
    "accuracy_score(yhat_validation5, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we get ??.??% on validation set.\n",
    "\n",
    "We got our best result with $k=3$ (??.??% > ??.??% > ??.??%), so let's get a final evaluation on our held-out test set. Predict `yhat_test3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### begin your code here (1 line).\n",
    "\n",
    "### end your code here.\n",
    "accuracy_score(yhat_test3, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on test data (??.??%) is both close to performance on validation and training data and is high.\n",
    "\n",
    "We have a good $k$-NN model now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
